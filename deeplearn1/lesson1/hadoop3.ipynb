{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require(['base/js/utils'], function(utils) {\n",
    "    utils.load_extensions('usability/ruler/main');\n",
    "    utils.load_extensions('usability/toc2/main');\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Installations 3: Ambari Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Apache Ambari in order to install a Hadoop cluster. If you already have your Hadoop cluster through some other installation (such as the previous notebook), you should not use this notebook.\n",
    "\n",
    "https://ambari.apache.org\n",
    "\n",
    "We'll prepare our Ambari master server for an Ambari installation and then we'll use its Admin GUI in order to perform the actual software installation. Afterwards, we'll update all of our support nodes to recognize the installation that was performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from aws_request import *\n",
    "from aws_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Spot Instance Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instances for the application were generated by the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "app_request = InstanceRequest('app')\n",
    "app_instances = app_request.get_fulfilled()\n",
    "\n",
    "app_host_names = [instance['PublicDnsName'] for instance in app_instances]\n",
    "app_host_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Server Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script can be used both for the initial cluster creation and to expand the capacity of a cluster after initial setup. If this is an initial setup, set `is_initial_setup` to `True`. Otherwise, set it to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_initial_setup = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After knowing whether this is an initial setup or not, the following decides whether Ambari and Jupyter need to be installed and identifies all the other nodes in the cluster. Please check the output to make sure you know which host names will be in your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ambari_host_name = None\n",
    "\n",
    "if is_initial_setup:\n",
    "    ambari_host_name = app_host_names[0]\n",
    "\n",
    "    print 'Ambari', ambari_host_name\n",
    "\n",
    "for host_name in app_host_names:\n",
    "    if host_name != ambari_host_name:\n",
    "        print 'Support', host_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Ambari Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Ambari requires identifying the version of Ambari we wish to install. At the time of this notebook's creation, the latest versions available from a public repository is version 2.2.2.0 (released April 29, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/install_ambari.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# Add Ambari release repository\n",
    "\n",
    "AMBARI_VERSION=2.2.2.0\n",
    "UBUNTU_REPO=http://public-repo-1.hortonworks.com/ambari/ubuntu14\n",
    "\n",
    "if [ ! -f /etc/apt/sources.list.d/ambari.list ]; then\n",
    "    curl -Ls $UBUNTU_REPO/2.x/updates/$AMBARI_VERSION/ambari.list | \\\n",
    "        sudo tee /etc/apt/sources.list.d/ambari.list\n",
    "\n",
    "    sudo apt-key adv --recv-keys \\\n",
    "        --keyserver keyserver.ubuntu.com B9733A7A07513CAD\n",
    "\n",
    "    sudo apt-get update\n",
    "fi\n",
    "\n",
    "# Install Ambari\n",
    "\n",
    "if [ \"\" == \"$(which ambari-server)\" ]; then\n",
    "    sudo apt-get install --no-install-recommends --yes ambari-server\n",
    "fi\n",
    "\n",
    "# Configure Ambari\n",
    "\n",
    "AMBARI_JAVA_HOME=$(grep java.home /etc/ambari-server/conf/ambari.properties | cut -d'=' -f 2)\n",
    "\n",
    "if [ \"/usr/lib/jvm/java-7-oracle\" != \"$AMBARI_JAVA_HOME\" ]; then\n",
    "    sudo ambari-server setup -j /usr/lib/jvm/java-7-oracle -s\n",
    "fi\n",
    "\n",
    "if [ \"\" == \"$(netstat -an | grep 8080)\" ]; then\n",
    "    sudo ambari-server start\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will install it only on the Ambari server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if ambari_host_name in app_host_names:\n",
    "    run_script('ubuntu', [ambari_host_name], 'install_ambari.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Admin GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, the next step is to use the Ambari user interface to configure your cluster. It may take a few seconds before the UI is actually available (you will get connection errors), but once it's up:\n",
    "\n",
    "* **username**: `admin`\n",
    "* **password**: `admin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Ambari Server:'\n",
    "print 'http://' + ambari_host_name + ':8080/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the steps are self-explanatory. The version of the Hortonworks Data Platform (HDP) you will want to install depends on the version of Ambari (usually you will want to install the latest). For Ambari 2.2.1, the latest HDP version is 2.4.\n",
    "\n",
    "Use the private host names for your EC2 instances when asked for the host names, with `ubuntu` as the user and the same private key file you have been using for SSH authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for instance in app_instances:\n",
    "    print instance['PrivateDnsName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install just the bare minimum services that you need (Hadoop, Yarn, Zookeeper) and allow Ambari to select where to install its master servers. When you reach the Slaves and Clients step, make sure all servers participate as a Data Node and have a Client installed.\n",
    "\n",
    "If you mounted extra volumes (the optional step from earlier in this notebook) and you are using an instance with local storage, it should have been automatically added on the Customize Services screen.\n",
    "\n",
    "Click the deploy button to proceed with the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognize Ambari Clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that Ambari is installed, we'll want all the servers in the cluster to know how to access the Hadoop and Spark clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/find_clients.sh\n",
    "#!/bin/bash\n",
    "source ~/.profile\n",
    "\n",
    "if [ ! -d /usr/hdp ]; then\n",
    "    echo Hortonworks Data Platform has not yet been deployed.\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Find Hadoop Streaming JAR\n",
    "\n",
    "if [ \"\" == \"$HADOOP_HOME\" ]; then\n",
    "    HADOOP_HOME=$(find /usr/hdp -maxdepth 2 -name hadoop)\n",
    "    HADOOP_STREAMING_JAR=$HADOOP_HOME/hadoop-streaming.jar\n",
    "\n",
    "    if [ ! -f \"$HADOOP_STREAMING_JAR\" ]; then\n",
    "        HADOOP_STREAMING_JAR=$(find /usr/hdp -name hadoop-streaming.jar)\n",
    "\n",
    "        if [ \"\" != \"$HADOOP_STREAMING_JAR\" ]; then\n",
    "            sudo ln -s $HADOOP_STREAMING_JAR $HADOOP_HOME\n",
    "        else\n",
    "            echo Please place hadoop-streaming.jar in $HADOOP_HOME first.\n",
    "            return\n",
    "        fi\n",
    "    fi\n",
    "\n",
    "    echo >> $HOME/.profile\n",
    "    echo \"# Added for MRJob\" >> $HOME/.profile\n",
    "    echo export HADOOP_HOME=\"$HADOOP_HOME\" >> $HOME/.profile\n",
    "fi\n",
    "\n",
    "# Find Spark installation\n",
    "\n",
    "if [ \"\" == \"$SPARK_HOME\" ]; then\n",
    "    if [ -d \"/usr/hdp/current/spark-client\" ]; then\n",
    "        echo >> $HOME/.profile\n",
    "        echo \"# Added for PySpark\" >> $HOME/.profile\n",
    "        echo export SPARK_HOME=/usr/hdp/current/spark-client >> $HOME/.profile\n",
    "    fi\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll do this on all the servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_script('ubuntu', app_host_names, 'find_clients.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to make sure that the proper directories exist on HDFS. We'll want the user home for the Ubuntu user, which is the default location where data is stored in an MRJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/init_hdfs.sh\n",
    "#!/bin/bash\n",
    "source ~/.profile\n",
    "\n",
    "sudo su -c \"$HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/ubuntu\" hdfs\n",
    "sudo su -c \"$HADOOP_HOME/bin/hdfs dfs -chown ubuntu:ubuntu /user/ubuntu\" hdfs\n",
    "\n",
    "sudo su -c \"$HADOOP_HOME/bin/hdfs dfs -mkdir -p /tmp\" hdfs\n",
    "sudo su -c \"$HADOOP_HOME/bin/hdfs dfs -chmod a+rwx /tmp\" hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we run the command on our designated Notebook server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_script('ubuntu', app_host_names[:1], 'init_hdfs.sh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:legacy]",
   "language": "python",
   "name": "conda-env-legacy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "toc_position": {
   "left": "1311.05px",
   "right": "20px",
   "top": "127px",
   "width": "327px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}