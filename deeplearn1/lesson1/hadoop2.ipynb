{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require(['base/js/utils'], function(utils) {\n",
    "    utils.load_extensions('usability/ruler/main');\n",
    "    utils.load_extensions('usability/toc2/main');\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Installations 2: Single Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook looks at installing a single node Hadoop cluster on a non-EMR server. If you plan on adding more nodes in the future, you should not use this notebook (skip to the Ambari cluster notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from aws_request import *\n",
    "from aws_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Spot Instance Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instances for the application were generated by the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "app_request = InstanceRequest('app')\n",
    "app_instances = app_request.get_fulfilled()\n",
    "\n",
    "app_host_names = [instance['PublicDnsName'] for instance in app_instances]\n",
    "app_host_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Configuration Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create a directory to store all the configuration files we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the individual files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile conf/core-site.xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.default.name</name>\n",
    "    <value>hdfs://localhost:9000</value>\n",
    "  </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile conf/yarn-site.xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n",
    "    <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n",
    "  </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile conf/mapred-site.xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "  </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile conf/hdfs-site.xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>1</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>NAME_NODE_FOLDERS</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>DATA_NODE_FOLDERS</value>\n",
    "  </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upload_file('ubuntu', app_host_names, 'conf/core-site.xml')\n",
    "upload_file('ubuntu', app_host_names, 'conf/yarn-site.xml')\n",
    "upload_file('ubuntu', app_host_names, 'conf/mapred-site.xml')\n",
    "upload_file('ubuntu', app_host_names, 'conf/hdfs-site.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a script which will install Hadoop binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/install_hadoop.sh\n",
    "#!/bin/bash\n",
    "source ~/.profile\n",
    "\n",
    "HADOOP_VERSION=2.7.3\n",
    "\n",
    "# Add the Hadoop user\n",
    "\n",
    "sudo addgroup hadoop\n",
    "sudo adduser --ingroup hadoop --disabled-password --gecos \"\" hduser\n",
    "\n",
    "# Download Hadoop\n",
    "\n",
    "wget -qq https://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\n",
    "tar -zxf hadoop-$HADOOP_VERSION.tar.gz\n",
    "sudo mv hadoop-$HADOOP_VERSION /usr/local/lib\n",
    "sudo chown -R hduser:hadoop /usr/local/lib/hadoop-$HADOOP_VERSION\n",
    "\n",
    "# Set HADOOP_HOME environment variable\n",
    "\n",
    "HADOOP_HOME=/usr/local/lib/hadoop-$HADOOP_VERSION\n",
    "\n",
    "echo >> $HOME/.profile\n",
    "echo \"# Added for MRJob\" >> $HOME/.profile\n",
    "echo export HADOOP_HOME=\"$HADOOP_HOME\" >> $HOME/.profile\n",
    "echo 'export PATH=$PATH:$HADOOP_HOME/bin' >> $HOME/.profile\n",
    "\n",
    "# Create name node and data node folders on mount points\n",
    "\n",
    "for folder in $(ls -1 /hadoop); do\n",
    "    sudo mkdir -p /hadoop/$folder/hdfs/namenode\n",
    "    sudo mkdir -p /hadoop/$folder/hdfs/datanode\n",
    "    sudo chown -R hduser:hadoop /hadoop/$folder\n",
    "done\n",
    "\n",
    "# Update configuration to use data node folders\n",
    "\n",
    "NAME_NODES=$(\n",
    "    ls -1 /hadoop | awk '{ print \"file:/hadoop/\" $1 \"/hdfs/namenode\" }' | tr '\\n' ','\n",
    ")\n",
    "\n",
    "sed -i -e \"s@NAME_NODE_FOLDERS@$NAME_NODES@g\" hdfs-site.xml\n",
    "\n",
    "DATA_NODES=$(\n",
    "    ls -1 /hadoop | awk '{ print \"file:/hadoop/\" $1 \"/hdfs/datanode\" }' | tr '\\n' ','\n",
    ")\n",
    "\n",
    "sed -i -e \"s@DATA_NODE_FOLDERS@$DATA_NODES@g\" hdfs-site.xml\n",
    "\n",
    "# Move all the -site.xml configuration files to the Hadoop folder\n",
    "\n",
    "sudo mv *-site.xml $HADOOP_HOME/etc/hadoop\n",
    "sudo chown hduser:hadoop $HADOOP_HOME/etc/hadoop/*-site.xml\n",
    "\n",
    "# Enable SSH for the hduser\n",
    "\n",
    "sudo su -c 'mkdir /home/hduser/.ssh' - hduser\n",
    "sudo su -c 'ssh-keygen -t rsa -P \"\" -f /home/hduser/.ssh/id_rsa' - hduser\n",
    "sudo su -c 'cp /home/hduser/.ssh/id_rsa.pub /home/hduser/.ssh/authorized_keys' - hduser\n",
    "\n",
    "sudo su -c 'ssh -o StrictHostKeyChecking=no localhost \"echo\"' - hduser\n",
    "sudo su -c 'ssh -o StrictHostKeyChecking=no 0.0.0.0 \"echo\"' - hduser\n",
    "\n",
    "# Update .profile for hduser\n",
    "\n",
    "sudo su -c \"echo export JAVA_HOME=$JAVA_HOME >> /home/hduser/.profile\" - hduser\n",
    "\n",
    "sudo su -c \"echo export HADOOP_INSTALL=$HADOOP_HOME >> /home/hduser/.profile\" - hduser\n",
    "sudo su -c \"echo export HADOOP_MAPRED_HOME=$HADOOP_HOME >> /home/hduser/.profile\" - hduser\n",
    "sudo su -c \"echo export HADOOP_COMMON_HOME=$HADOOP_HOME >> /home/hduser/.profile\" - hduser\n",
    "sudo su -c \"echo export HADOOP_HDFS_HOME=$HADOOP_HOME >> /home/hduser/.profile\" - hduser\n",
    "sudo su -c \"echo export YARN_HOME=$HADOOP_HOME >> /home/hduser/.profile\" - hduser\n",
    "sudo su -c \"echo 'export PATH=$HADOOP_HOME/bin:$PATH' >> /home/hduser/.profile\" - hduser\n",
    "\n",
    "sudo sed -i -e \"s@\\${JAVA_HOME}@$JAVA_HOME@g\" \\\n",
    "    /usr/local/lib/hadoop-$HADOOP_VERSION/etc/hadoop/hadoop-env.sh\n",
    "\n",
    "# Format the name node and start DFS and Yarn\n",
    "\n",
    "sudo su -c \"$HADOOP_HOME/bin/hdfs namenode -format\" - hduser\n",
    "sudo su -c \"$HADOOP_HOME/sbin/start-dfs.sh\" - hduser\n",
    "sudo su -c \"$HADOOP_HOME/sbin/start-yarn.sh\" - hduser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script on all servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_script('ubuntu', app_host_names, 'install_hadoop.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/install_spark.sh\n",
    "#!/bin/bash\n",
    "source ~/.profile\n",
    "\n",
    "SPARK_VERSION=1.6.3\n",
    "\n",
    "# Download Spark\n",
    "\n",
    "wget -qq http://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-without-hadoop.tgz\n",
    "tar -zxf spark-$SPARK_VERSION-bin-without-hadoop.tgz\n",
    "sudo mv spark-$SPARK_VERSION-bin-without-hadoop /usr/local/lib/spark-$SPARK_VERSION\n",
    "\n",
    "# Set HADOOP_HOME environment variable\n",
    "\n",
    "SPARK_HOME=/usr/local/lib/spark-$SPARK_VERSION\n",
    "\n",
    "echo >> $HOME/.profile\n",
    "echo \"# Added for Spark\" >> $HOME/.profile\n",
    "echo export SPARK_HOME=\"$SPARK_HOME\" >> $HOME/.profile\n",
    "echo 'export PATH=$PATH:$SPARK_HOME/bin' >> $HOME/.profile\n",
    "\n",
    "# Update spark-env.sh with Java and Hadoop information\n",
    "\n",
    "echo export SPARK_DIST_CLASSPATH=$(hadoop classpath) >> $SPARK_HOME/conf/spark-env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script on all servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_script('ubuntu', app_host_names, 'install_spark.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to make sure that the proper directories exist on HDFS. We'll want the user home for the Ubuntu user, which is the default location where data is stored in an MRJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/init_hdfs.sh\n",
    "#!/bin/bash\n",
    "source ~/.profile\n",
    "\n",
    "sudo su -c \"$HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/ubuntu\" hduser\n",
    "sudo su -c \"$HADOOP_HOME/bin/hdfs dfs -chown ubuntu:ubuntu /user/ubuntu\" hduser\n",
    "\n",
    "sudo su -c \"$HADOOP_HOME/bin/hdfs dfs -mkdir -p /tmp\" hduser\n",
    "sudo su -c \"$HADOOP_HOME/bin/hdfs dfs -chmod a+rwx /tmp\" hduser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we run the command on our designated Notebook server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_script('ubuntu', app_host_names[:1], 'init_hdfs.sh')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:legacy]",
   "language": "python",
   "name": "conda-env-legacy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "toc_position": {
   "left": "1311.05px",
   "right": "20px",
   "top": "127px",
   "width": "327px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
