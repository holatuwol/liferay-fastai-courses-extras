{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Amazon Basics 3: Basic Scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Usage Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The purpose of this notebook is to alleviate some of the boilerplate code associated with file management related to uploading and downloading files to/from an Amazon Elastic Compute Cloud instance as well as boilerplate code associated with running installation scripts on multiple servers (essentially, management tasks usually performed by tools like Puppet or Chef)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from aws_base import *\n",
    "from IPython.utils.py3compat import *\n",
    "import os\n",
    "import pysftp\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we'll update our user to give them permissions to read information from Identity and Access Management.\n",
    "\n",
    "https://console.aws.amazon.com/iam/home#users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Update Your User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we'll update our user to give them permissions to read information about Amazon EC2 instances.\n",
    "\n",
    "https://console.aws.amazon.com/iam/home#users\n",
    "\n",
    "Select the user that you created in the previous notebook and click on the **Add Permissions** button. On the permissions screen, select **Attach existing policies directly** and add ``AmazonEC2ReadOnlyAccess``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Specify Private Key File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When creating an EC2 instance, you need to specify which private key you would like to use in order to access the EC2 instance. If you don't have a private key yet, please create one in this region.\n",
    "\n",
    "http://console.aws.amazon.com/ec2/v2/home#KeyPairs\n",
    "\n",
    "The scripts will need to use your private key in order to transfer files to the servers. Please set the on-disk location of your private key here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "private_key_name = 'mdang-training-shared'\n",
    "private_key_location = '~/Dropbox/Amazon/mdang-training-shared.pem'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will want to confirm that the file exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "private_key_location = os.path.expanduser(private_key_location)\n",
    "assert private_key_location is not None and os.path.isfile(private_key_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since SSH keys are not shared across regions, the following makes sure that the private key that you have specified above matches one of the keys that is in the region we'll be using for our commands, which we've specified in `aws configure`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "private_key_json = aws(\n",
    "    'ec2', 'describe-key-pairs', '--key-names', private_key_name)\n",
    "\n",
    "private_keys = None\n",
    "\n",
    "if private_key_json is not None:\n",
    "    private_keys = private_key_json['KeyPairs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that the command this time is `ec2`, and that we're doing a read command. This is the reason that we wanted the `AmazonEC2ReadOnlyAccess` permission on our user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Utility Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Local Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This series of notebooks creates a lot of intermediate files. In order to reduce clutter in the local folder, we'll create those intermediate files in special folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('awscli'):\n",
    "    os.mkdir('awscli')\n",
    "\n",
    "if not os.path.isdir('output'):\n",
    "    os.mkdir('output')\n",
    "\n",
    "if not os.path.isdir('scripts'):\n",
    "    os.mkdir('scripts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Backwards Compatible check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility method which calls subprocess.check_output and wraps the result\n",
    "in bytes_to_str() and strips the extra whitespace\n",
    "\"\"\"\n",
    "def check_output(args):\n",
    "    return bytes_to_str(subprocess.check_output(args)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Check Known Hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Most EC2 commands only work from a notebook if the server is recognized as a known host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility method to determine if the host is registered in the known_hosts file.\n",
    "Uses ssh-keygen directly to determine this.\n",
    "\"\"\"\n",
    "def is_known_host(host_name):\n",
    "    is_known_host_name = subprocess.call(['ssh-keygen', '-F', host_name])\n",
    "    return is_known_host_name == 0\n",
    "\n",
    "\"\"\"\n",
    "Utility method to add a known host to the known_hosts file.\n",
    "\"\"\"\n",
    "def add_known_host(host_name, key_type, key_value):\n",
    "    remove_known_host(host_name)\n",
    "\n",
    "    with open(os.path.expanduser('~/.ssh/known_hosts'), 'a') as known_hosts:\n",
    "        known_hosts.write('%s %s %s' % (host_name, key_type, key_value))\n",
    "\n",
    "    print('%s added as known host' % host_name)\n",
    "\n",
    "\"\"\"\n",
    "Utility method to remove a known host from the known_hosts file.\n",
    "\"\"\"\n",
    "def remove_known_host(host_name):\n",
    "    if is_known_host(host_name):\n",
    "        subprocess.call(['ssh-keygen', '-R', host_name])\n",
    "        print('%s removed from known hosts' % host_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Upload Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some steps will require uploading a file to all servers. The following is a wrapper which checks the file size in order to determine whether Amazon S3 should be used as an intermediary (whenever the file is larger than 1 MB) or if it is better to simply upload the file to the server(s) directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility method which uploads the given file via sFTP or S3 depending on size.\n",
    "\"\"\"\n",
    "def upload_file(user_name, host_names, source_file_name, target_file_name = None):\n",
    "    file_size = os.path.getsize(source_file_name)\n",
    "\n",
    "    if target_file_name is None:\n",
    "        target_file_name = os.path.basename(source_file_name)\n",
    "\n",
    "    if file_size < 1024 * 1024:\n",
    "        upload_file_sftp(user_name, host_names, source_file_name, target_file_name)\n",
    "    else:\n",
    "        upload_file_s3(user_name, host_names, source_file_name, target_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Upload File - SFTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility method which uploads the given file via sFTP to multiple servers.\n",
    "\"\"\"\n",
    "def upload_file_sftp(user_name, host_names, source_file_name, target_file_name):\n",
    "    global private_key_location\n",
    "\n",
    "    print('Uploading %s to %d servers' % (source_file_name, len(host_names)))\n",
    "\n",
    "    for host_name in host_names:\n",
    "        if not is_known_host(host_name):\n",
    "            print('%s is not a known host' % host_name)\n",
    "            continue\n",
    "\n",
    "        with pysftp.Connection(\n",
    "            host_name, username = user_name,\n",
    "            private_key = private_key_location) as sftp:\n",
    "\n",
    "            sftp.put(source_file_name, target_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Upload File - S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some steps will require uploading a large file to all servers. We can use our parallel script runner in order to accomplish this by using S3 as an intermediary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility method which copies a file to an S3 bucket (with a mostly-unique ID) and\n",
    "downloads the file onto multiple servers. Optionally can be used to upload a\n",
    "file to S3 but not to any servers by passing an empty list of host names.\n",
    "\"\"\"\n",
    "\n",
    "def upload_file_s3(user_name, host_names, source_file_name, target_file_name):\n",
    "    with open('awscli/bucket.txt', 'r') as bucket_file:\n",
    "        bucket_name = bucket_file.read().strip()\n",
    "\n",
    "    target_file_path = 's3://%s/%s' % (bucket_name, target_file_name)\n",
    "\n",
    "    if host_names is not None and len(host_names) > 0:\n",
    "        local_host_name = check_output(['hostname', '-s'])\n",
    "        local_timestamp = check_output(['date', '+%s'])\n",
    "\n",
    "        suffix = '.' + local_host_name + '.' + local_timestamp\n",
    "        target_file_path += suffix\n",
    "\n",
    "    aws('s3', 'cp', source_file_name, target_file_path)\n",
    "\n",
    "    if host_names is None or len(host_names) == 0:\n",
    "        return\n",
    "\n",
    "    script_file_name = 'download_from_s3.sh'\n",
    "\n",
    "    with open('scripts/' + script_file_name, 'w') as script_file:\n",
    "        script_file.write('#!/bin/bash\\n')\n",
    "        script_file.write('source ~/.profile\\n')\n",
    "        script_file.write('aws s3 cp %s %s' % (target_file_path, target_file_name))\n",
    "\n",
    "    run_script(user_name, host_names, script_file_name)\n",
    "    aws('s3', 'rm', target_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Run Remote Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Many of the things we build involve creating small shell scripts which we then execute on all servers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility method which uploads a file in the scripts folder to multiple servers\n",
    "and prepares a script that will run it on all servers.\n",
    "\"\"\"\n",
    "def run_script(user_name, host_names, script_name):\n",
    "    global private_key_location\n",
    "\n",
    "    upload_file(user_name, host_names, 'scripts/' + script_name)\n",
    "\n",
    "    with open('scripts/run_script.sh', 'w') as script_file:\n",
    "        for host_name in host_names:\n",
    "            if not is_known_host(host_name):\n",
    "                print('%s is not a known host' % host_name)\n",
    "                continue\n",
    "\n",
    "            # Ensure that the file has execute permissions on the host\n",
    "\n",
    "            script_file.write('ssh -i %s %s@%s \"chmod u+x %s\"\\n' % \\\n",
    "                (private_key_location, user_name, host_name, script_name))\n",
    "\n",
    "            # Execute the script on the host, but log to a local file to avoid\n",
    "            # filling up the notebook with text. Also run each script in the\n",
    "            # background since the nodes do not depend on each other.\n",
    "\n",
    "            output_log = 'output/' + script_name + '.' + host_name + '.log'\n",
    "\n",
    "            script_file.write('ssh -i %s %s@%s \"./%s\" > %s 2>&1 &\\n' % \\\n",
    "                (private_key_location, user_name, host_name, script_name, output_log))\n",
    "\n",
    "        # Wait for all background processes to finish.\n",
    "\n",
    "        script_file.write('wait\\n')\n",
    "\n",
    "    print('Executing %s on %d servers' % (script_name, len(host_names)))\n",
    "\n",
    "    subprocess.call(['chmod', 'u+x', 'scripts/run_script.sh'])\n",
    "    subprocess.call('scripts/run_script.sh', shell = True)\n",
    "\n",
    "    print('Completed %s on %d servers' % (script_name, len(host_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Upload Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some steps will require uploading a folder to all servers. The following is a wrapper which transforms the folder into an archive and uploads the archive to all servers. Note that it will not auto-extract the archive, and relies on the caller to do so afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility method which creates an of the specified folder and uploads it to\n",
    "all servers using the same name as the folder.\n",
    "\"\"\"\n",
    "def upload_archive(user_name, host_names, source_folder_name):\n",
    "    archive_name = os.path.basename(source_folder_name) + '.tar.gz'\n",
    "    subprocess.call(['tar', '-acf', archive_name, source_folder_name])\n",
    "    upload_file(user_name, host_names, archive_name)\n",
    "    os.remove(archive_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Install AWS CLI Remotely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to access our S3 buckets, we'll need to install AWS CLI. The downside is the AWS CLI that is available from the Ubuntu repositories is out of date. Therefore, we'll need to install it using Miniconda. Additionally, we'll assume there's an `s3_bucket.json` which tells us which region we should use as well as our default bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/install_awscli.sh\n",
    "#!/bin/bash\n",
    "\n",
    "if [ \"\" != \"$(uname -a | grep Ubuntu)\" ]; then\n",
    "    # Add mirror to avoid slow downloads\n",
    "\n",
    "    APT_MIRROR=\"mirror://mirrors.ubuntu.com/mirrors.txt\"\n",
    "    APT_REPOSITORIES=\"main restricted universe multiverse\"\n",
    "\n",
    "    #sudo sed -i -e \"s@[^ ]*ec2.archive.ubuntu.com[^ ]*@$APT_MIRROR@g\" \\\n",
    "    #    /etc/apt/sources.list\n",
    "\n",
    "    sudo apt-get update\n",
    "\n",
    "    # Update Python SSL libraries\n",
    "\n",
    "    sudo apt-get -y install gcc libffi-dev libssl-dev python-dev\n",
    "else\n",
    "    sudo yum -y update\n",
    "    sudo yum -y install gcc libffi-devel libssl-devel openssl-devel python-devel\n",
    "\n",
    "fi\n",
    "\n",
    "# Install pip\n",
    "wget --quiet https://bootstrap.pypa.io/get-pip.py\n",
    "sudo -H python get-pip.py\n",
    "\n",
    "sudo -H $(which pip) install --upgrade ndg-httpsclient\n",
    "sudo -H $(which pip) install awscli\n",
    "\n",
    "# Set defaults on AWS configuration\n",
    "\n",
    "EC2_REGION=$(cat region.txt | cut -d'\"' -f 4)\n",
    "\n",
    "echo \"\n",
    "\n",
    "$EC2_REGION\n",
    "json\" | aws configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Leverage our parallel script runner in order to install AWS CLI on all machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility method which installs AWS CLI on all machines and configures its default\n",
    "region to be the same as the region for the local machine.\n",
    "\"\"\"\n",
    "def install_awscli(user_name, host_names):\n",
    "    global region\n",
    "\n",
    "    with open('awscli/region.txt', 'w') as region_file:\n",
    "        region_file.write(region)\n",
    "\n",
    "    upload_file(user_name, host_names, 'awscli/region.txt')\n",
    "    run_script(user_name, host_names, 'install_awscli.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Set Default Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All future notebooks assume that you have access to an S3 bucket that stores files used by installers or files that need to be uploaded/downloaded during the installation process.\n",
    "\n",
    "http://console.aws.amazon.com/s3/home\n",
    "\n",
    "In order to allow it to be a variable in these installers, it's useful to be able to set it as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/set_bucket.sh\n",
    "#!/bin/bash\n",
    "\n",
    "touch $HOME/.profile\n",
    "\n",
    "S3_BUCKET=$(cat bucket.txt)\n",
    "echo >> $HOME/.profile\n",
    "echo \"# Added for AWS CLI\" >> $HOME/.profile\n",
    "echo \"export S3_BUCKET=$S3_BUCKET\" >> $HOME/.profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Provide a utility method which sets the `S3_BUCKET` environment variable on all hosts, assuming it is in the correct region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility method which sets an environment variable specifying a bucket which can\n",
    "be referenced in other scripts.\n",
    "\"\"\"\n",
    "def set_bucket(user_name, host_names, bucket_name):\n",
    "    global region\n",
    "\n",
    "    bucket_url = 'http://s3-%s.amazonaws.com/%s/' % (region, bucket_name)\n",
    "    check_bucket_url = check_output(['curl', '-s', bucket_url])\n",
    "\n",
    "    if check_bucket_url.find('PermanentRedirect') != -1:\n",
    "        print('Bucket is not in region %s')\n",
    "\n",
    "    with open('awscli/bucket.txt', 'w') as bucket_file:\n",
    "        bucket_file.write(bucket_name)\n",
    "\n",
    "    upload_file(user_name, host_names, 'awscli/bucket.txt')\n",
    "    run_script(user_name, host_names, 'set_bucket.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Convert Notebook to Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following cell will use `jupyter nbconvert` to build an `aws_util.py` which will be used in future notebooks in this series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var script_file = 'aws_util.py';\n",
    "\n",
    "var notebook_name = window.document.getElementById('notebook_name').innerHTML;\n",
    "var nbconvert_command = 'jupyter nbconvert --stdout --to script ' + notebook_name;\n",
    "\n",
    "var grep_command = \"grep -v '^#' | grep -v -F get_ipython | sed '/^$/N;/^\\\\n$/D'\";\n",
    "var command = '!' + nbconvert_command + ' | ' + grep_command + ' > ' + script_file;\n",
    "\n",
    "if (Jupyter.notebook.kernel) {\n",
    "    Jupyter.notebook.kernel.execute(command);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
